import os
import glob
import torch
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoImageProcessor,
    TimesformerForVideoClassification,
    TimesformerConfig,
)
from decord import VideoReader, cpu
import numpy as np
from tqdm import tqdm
import evaluate  # For accuracy metric
import matplotlib.pyplot as plt  # Added for plotting

# --- Configuration ---
VIDEO_ROOT_DIR = "/home/elicer/yt-8m"  # Root directory of your dataset
TRAIN_DIR = os.path.join(VIDEO_ROOT_DIR, "train")
VAL_DIR = os.path.join(VIDEO_ROOT_DIR, "validation")
PRETRAINED_MODEL_NAME = "facebook/timesformer-base-finetuned-k400"

NUM_CLASSES = 10
BATCH_SIZE = 8
NUM_EPOCHS = 10  # Increase for better plots, e.g., 10 or 20
LEARNING_RATE = 3e-5
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
PLOT_FILENAME = "training_metrics.png"  # Name of the output plot file


# --- Video Dataset Class (Modified __getitem__ method) ---
class VideoClassificationDataset(Dataset):
    def __init__(self, video_dir, image_processor, num_frames, class_to_idx=None):
        self.video_dir = video_dir
        self.image_processor = image_processor
        self.num_frames = num_frames  # This should be model_config.num_frames
        self.video_files = []
        self.labels = []

        if class_to_idx is None:
            self.class_names = sorted(
                [d.name for d in os.scandir(video_dir) if d.is_dir()]
            )
            self.class_to_idx = {
                cls_name: i for i, cls_name in enumerate(self.class_names)
            }
        else:
            self.class_to_idx = class_to_idx
            self.class_names = sorted(list(class_to_idx.keys()))

        for class_name, label_idx in self.class_to_idx.items():
            class_path = os.path.join(video_dir, class_name)
            for video_file in glob.glob(os.path.join(class_path, "*.mp4")):
                self.video_files.append(video_file)
                self.labels.append(label_idx)

    def __len__(self):
        return len(self.video_files)

    def _sample_frames(self, video_path):
        try:
            # Using a default size for dummy frames if video reading fails.
            # This size should ideally match the processor's expected input size or be resizable by it.
            dummy_frame_height, dummy_frame_width = 224, 224  # Common default
            # Attempt to get actual size from processor if possible, though it's complex here.
            # For now, using a fixed common default for dummy frames.

            vr = VideoReader(video_path, ctx=cpu(0))
            total_frames = len(vr)
            if total_frames == 0:
                # Try to re-initialize, sometimes helps with network drives or temporary issues
                vr = VideoReader(video_path, ctx=cpu(0))
                total_frames = len(vr)
                if total_frames == 0:
                    print(
                        f"Warning: Video {video_path} has 0 frames even after retry. Returning dummy frames."
                    )
                    return [
                        np.zeros(
                            (dummy_frame_height, dummy_frame_width, 3), dtype=np.uint8
                        )
                        for _ in range(self.num_frames)
                    ]

            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
            frames = vr.get_batch(indices).asnumpy()
            # Ensure frames are not empty and have correct basic structure (H,W,C)
            if (
                frames.size == 0 or frames.ndim != 4 or frames.shape[3] != 3
            ):  # Expecting (NF, H, W, C)
                print(
                    f"Warning: Video {video_path} yielded problematic frames (shape: {frames.shape}). Returning dummy frames."
                )
                return [
                    np.zeros((dummy_frame_height, dummy_frame_width, 3), dtype=np.uint8)
                    for _ in range(self.num_frames)
                ]

            return list(frames)
        except Exception as e:
            print(
                f"Error reading or sampling frames from {video_path}: {e}. Returning dummy frames."
            )
            return [
                np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(self.num_frames)
            ]  # Fallback dummy frames

    def __getitem__(self, idx):
        video_path = self.video_files[idx]
        label = self.labels[idx]

        frames_list_hwc = self._sample_frames(video_path)  # List of HWC numpy arrays

        processed_output = self.image_processor(frames_list_hwc, return_tensors="pt")
        pixel_values = processed_output.pixel_values

        # Squeeze if an unnecessary batch dimension was added by the processor
        if pixel_values.ndim == 5 and pixel_values.shape[0] == 1:
            pixel_values = pixel_values.squeeze(0)  # Now pixel_values should be 4D

        # Ensure tensor is 4D before attempting 4D permutations
        if pixel_values.ndim != 4:
            # This can happen if _sample_frames returned dummy frames that weren't processed correctly
            # or if the processor behaved very unexpectedly.
            error_message = (
                f"Critical Error: pixel_values for video {video_path} has {pixel_values.ndim} dimensions "
                f"(shape: {pixel_values.shape}) after processor and potential squeeze. Expected 4D. "
                f"This might be due to issues reading the video or with the dummy frames fallback. "
                f"Frames list length: {len(frames_list_hwc)}, first frame shape if available: {frames_list_hwc[0].shape if frames_list_hwc else 'N/A'}."
            )
            print(error_message)  # Print error for easier debugging from logs
            # To make it fail hard and not proceed with a wrongly shaped tensor:
            raise ValueError(error_message)
            # Alternatively, return a specific error indicator or skip, but raising helps debug.

        # Now, pixel_values is 4D.
        # The model expects input of shape (num_frames, num_channels, height, width) per item.

        # Case 1: Input is (num_channels, num_frames, H, W)
        # Example: shape (3, 8, 224, 224) for num_frames=8
        if (
            pixel_values.shape[1] == self.num_frames
            and pixel_values.shape[0] != self.num_frames
        ):
            pixel_values = pixel_values.permute(
                1, 0, 2, 3
            )  # Convert to (num_frames, num_channels, H, W)

        # Case 2: Input is already (num_frames, num_channels, H, W)
        # Example: shape (8, 3, 224, 224) for num_frames=8
        elif pixel_values.shape[0] == self.num_frames:
            # Assuming if the first dimension is num_frames, it's likely (NF, C, H, W)
            # (VideoMAEImageProcessor usually places channels before H, W)
            pass  # Already in the correct format

        # Case 3: Unhandled 4D shape
        else:
            # This condition is met if the 4D tensor's dimensions don't match the expected patterns
            # involving self.num_frames in either the first or second dimension as anticipated.
            print(
                f"Warning: pixel_values for video {video_path} has an unhandled 4D shape: {pixel_values.shape} "
                f"(num_frames expected: {self.num_frames}). "
                "The model expects input of shape (num_frames, num_channels, height, width) per item. "
                "Please check video integrity and processor output alignment."
            )
            # Depending on strictness, you might raise an error or try a fallback.
            # For now, it will pass through with the potentially incorrect shape, which might cause issues later.

        return {
            "pixel_values": pixel_values,
            "labels": torch.tensor(label, dtype=torch.long),
        }


# --- Function to plot metrics ---
def plot_and_save_metrics(
    epochs_list, train_losses, val_losses, val_accuracies, filename
):
    fig, ax1 = plt.subplots(figsize=(10, 6))

    # Plotting training and validation loss
    color = "tab:red"
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss", color=color)
    ax1.plot(
        epochs_list,
        train_losses,
        color=color,
        linestyle="-",
        marker="o",
        label="Training Loss",
    )
    ax1.plot(
        epochs_list,
        val_losses,
        color=color,
        linestyle="--",
        marker="x",
        label="Validation Loss",
    )
    ax1.tick_params(axis="y", labelcolor=color)
    ax1.legend(loc="upper left")

    # Creating a second y-axis for validation accuracy
    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
    color = "tab:blue"
    ax2.set_ylabel("Accuracy", color=color)  # we already handled the x-label with ax1
    ax2.plot(
        epochs_list,
        val_accuracies,
        color=color,
        linestyle=":",
        marker="s",
        label="Validation Accuracy",
    )
    ax2.tick_params(axis="y", labelcolor=color)
    ax2.legend(loc="upper right")

    fig.tight_layout()  # otherwise the right y-label is slightly clipped
    plt.title("Training and Validation Metrics")
    plt.savefig(filename)
    print(f"Metrics plot saved to {filename}")
    plt.close(fig)  # Close the figure to free memory


# --- Main Training Script ---
if __name__ == "__main__":
    print(f"Using device: {DEVICE}")

    # 1. Load Image Processor and Model Configuration
    print(f"Loading image processor for {PRETRAINED_MODEL_NAME}...")
    image_processor = AutoImageProcessor.from_pretrained(PRETRAINED_MODEL_NAME)

    print(f"Loading model config for {PRETRAINED_MODEL_NAME}...")
    model_config = TimesformerConfig.from_pretrained(PRETRAINED_MODEL_NAME)
    NUM_FRAMES_TO_SAMPLE = model_config.num_frames
    print(f"Model expects {NUM_FRAMES_TO_SAMPLE} frames per video.")

    # 2. Create Datasets and DataLoaders
    print("Creating datasets...")
    train_dataset = VideoClassificationDataset(
        video_dir=TRAIN_DIR,
        image_processor=image_processor,
        num_frames=NUM_FRAMES_TO_SAMPLE,
    )
    val_dataset = VideoClassificationDataset(
        video_dir=VAL_DIR,
        image_processor=image_processor,
        num_frames=NUM_FRAMES_TO_SAMPLE,
        class_to_idx=train_dataset.class_to_idx,
    )

    print(
        f"Found {len(train_dataset.class_names)} classes: {train_dataset.class_names}"
    )
    if len(train_dataset.class_names) != NUM_CLASSES:
        print(
            f"Warning: Expected {NUM_CLASSES} classes, but found {len(train_dataset.class_names)} in {TRAIN_DIR}. Using found number."
        )
        # NUM_CLASSES = len(train_dataset.class_names) # Using actual found classes

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
    )
    print("DataLoaders created.")

    # 3. Initialize Model, Optimizer, Criterion
    print(f"Loading pre-trained model: {PRETRAINED_MODEL_NAME}...")
    model = TimesformerForVideoClassification.from_pretrained(
        PRETRAINED_MODEL_NAME,
        num_labels=len(train_dataset.class_names),
        ignore_mismatched_sizes=True,
    ).to(DEVICE)
    print("Model loaded.")

    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    criterion = (
        torch.nn.CrossEntropyLoss()
    )  # Not directly used if model calculates loss internally
    accuracy_metric = evaluate.load("accuracy")

    # Lists to store metrics for plotting
    history_train_losses = []
    history_val_losses = []
    history_val_accuracies = []
    epochs_ran = []

    # 4. Training Loop
    print("Starting training...")
    for epoch in range(NUM_EPOCHS):
        epochs_ran.append(epoch + 1)
        # --- Training Phase ---
        model.train()
        train_loss_epoch = 0.0
        train_accuracy_calculator = evaluate.load(
            "accuracy"
        )  # New calculator for each epoch's train acc
        progress_bar_train = tqdm(
            train_loader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS} [Train]"
        )

        for batch_idx, batch in enumerate(progress_bar_train):
            inputs = batch["pixel_values"].to(DEVICE)
            labels = batch["labels"].to(DEVICE)

            optimizer.zero_grad()
            outputs = model(pixel_values=inputs, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            loss.backward()
            optimizer.step()

            train_loss_epoch += loss.item()
            predictions = torch.argmax(logits, dim=-1)
            train_accuracy_calculator.add_batch(
                predictions=predictions, references=labels
            )

            if batch_idx % 20 == 0:
                progress_bar_train.set_postfix(
                    {
                        "loss": loss.item(),
                        "avg_loss": train_loss_epoch / (batch_idx + 1),
                    }
                )

        avg_train_loss = train_loss_epoch / len(train_loader)
        train_accuracy = train_accuracy_calculator.compute()["accuracy"]
        history_train_losses.append(avg_train_loss)
        print(
            f"Epoch {epoch + 1} [Train] - Avg Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.4f}"
        )

        # --- Validation Phase ---
        model.eval()
        val_loss_epoch = 0.0
        val_accuracy_calculator = evaluate.load(
            "accuracy"
        )  # New calculator for each epoch's val acc
        progress_bar_val = tqdm(
            val_loader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS} [Val]"
        )

        with torch.no_grad():
            for batch in progress_bar_val:
                inputs = batch["pixel_values"].to(DEVICE)
                labels = batch["labels"].to(DEVICE)

                outputs = model(pixel_values=inputs, labels=labels)
                loss = outputs.loss
                logits = outputs.logits

                val_loss_epoch += loss.item()
                predictions = torch.argmax(logits, dim=-1)
                val_accuracy_calculator.add_batch(
                    predictions=predictions, references=labels
                )
                progress_bar_val.set_postfix({"loss": loss.item()})

        avg_val_loss = val_loss_epoch / len(val_loader)
        val_accuracy = val_accuracy_calculator.compute()["accuracy"]
        history_val_losses.append(avg_val_loss)
        history_val_accuracies.append(val_accuracy)
        print(
            f"Epoch {epoch + 1} [Val]   - Avg Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}"
        )
        print("-" * 50)

        # Plot and save metrics at the end of each epoch (or after N epochs)
        # For now, plotting at the end of all epochs

    # Plot and save metrics after all epochs are done
    if NUM_EPOCHS > 0:
        plot_and_save_metrics(
            epochs_ran,
            history_train_losses,
            history_val_losses,
            history_val_accuracies,
            PLOT_FILENAME,
        )

    print("Training finished.")

    # Optional: Save the fine-tuned model
    output_dir = "./timesformer_finetuned_custom"
    model.save_pretrained(output_dir)
    image_processor.save_pretrained(output_dir)
    print(f"Model saved to {output_dir}")
